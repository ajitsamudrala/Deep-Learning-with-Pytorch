{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ajit\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import torch.nn.utils.rnn as utils\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:/Users/ajit/Anaconda3/Scripts/files/reviews.csv', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[~df['Comments / Feedback'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating']=df['Please rate the overall quality of the repair.'].apply(lambda x: 1 if x>3 else 0) #Converting the rating to a binary outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    904\n",
       "0    309\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(num_words=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(df['Comments / Feedback'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=tokenizer.texts_to_sequences(df['Comments / Feedback']) #tokenizing the input sequences. Keras Tokenizer also takes care of Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[sequence[:200] for sequence in X] #only considering first 200 words of the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"C:/Users/ajit/Downloads/Compressed/glove50d.txt\",encoding=\"utf8\") #importing Glove vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index={}\n",
    "for line in file:\n",
    "    values=line.split()\n",
    "    word=values[0]\n",
    "    embeddings=torch.from_numpy(np.asarray(values[1:],dtype='float32')).view(1,-1).float()\n",
    "    embeddings_index[word]=embeddings\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix={}\n",
    "for word in word_index.keys():\n",
    "    if word in embeddings_index.keys():\n",
    "        embeddings_matrix[word_index[word]]=embeddings_index[word]\n",
    "    else:\n",
    "        embeddings_matrix[word_index[word]]=torch.zeros(1,50,dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sequence in X:\n",
    "    for i in range(len(sequence)):\n",
    "        sequence[i]=embeddings_matrix[sequence[i]] #replacing the tokens with corrsponding dense vector from Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[torch.cat(sequence,dim=0) for sequence in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=[float(y) for y in df['rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,random_state=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size:14px; color:black;\">\n",
    "  <span style=\"color:red;font-weight:bold;\">Customdatasets</span> need to subclass \"Dataset\" class in utils module. All subclasess of <a href=\"https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#Dataset\"><span style=\"color: orange;font-weight:bold;\">Dataset</span></a> should override __len__ and __getitem__ methods of the parent class. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customDataset(data_utils.Dataset):\n",
    "    \n",
    "    def __init__(self,x,y):\n",
    "        self.x=x\n",
    "        self.y=y\n",
    "        self.lengths=[len(sequence) for sequence in self.sorting()]\n",
    "        self.padded_x=self.padding()\n",
    "    def padding(self):\n",
    "        return utils.pad_sequence(self.sorting(),batch_first=True)\n",
    "    def sorting(self):\n",
    "        return sorted(self.x,key=len,reverse=True)\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self,id):\n",
    "        return (self.padded_x[id],self.y[id],self.lengths[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset=customDataset(X_train,torch.tensor(y_train).view(-1,1)) #creating an object of the customedataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader=data_utils.DataLoader(new_dataset,batch_size=5,drop_last=True) #creating a generator for batch processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset=customDataset(X_test,torch.tensor(y_test).view(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader=data_utils.DataLoader(test_dataset,batch_size=5,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "181"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am using a deep bidirectional LSTM with 3 layers followed by a dense layer and output layer with sigmoid activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLSTM(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_dim,hidden_dim,num_layers,dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.num_layers=num_layers\n",
    "        self.lstm=nn.LSTM(input_dim,hidden_dim,num_layers,bias=True,batch_first=True,dropout=dropout,bidirectional=True)\n",
    "        self.fc1=nn.Linear(hidden_dim*2,10)\n",
    "        self.fc2=nn.Linear(10,1)\n",
    "        self.batchnorm=nn.BatchNorm1d(10)\n",
    "        \n",
    "    def forward(self,seq,lengths):\n",
    "        bs=seq.size(0)\n",
    "        self.h=self.init_hidden(bs)\n",
    "        inputs=utils.pack_padded_sequence(seq,lengths,batch_first=True) #to pack the variable lengths in the batch to feed it to LSTM\n",
    "        output,last_layer=self.lstm(inputs,self.h)\n",
    "        output,lengths=utils.pad_packed_sequence(output,batch_first=True) #collecting the output from the LSTM and padding it\n",
    "        output=self.fc1(output[:,-1,:])\n",
    "        output=F.dropout(F.relu(self.batchnorm(output)),p=0.1,training=True)\n",
    "        output=self.fc2(output)\n",
    "        output=F.sigmoid(output)\n",
    "        return output\n",
    "    \n",
    "    def init_hidden(self,batch_size): #function for hidden state initializations\n",
    "        return (torch.zeros(self.num_layers*2, batch_size,self.hidden_dim),\n",
    "                torch.zeros(self.num_layers*2,batch_size, self.hidden_dim))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=DeepLSTM(50,40,3,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "criteria=nn.BCELoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     8] loss: 0.711\n",
      "[1,    16] loss: 0.674\n",
      "[1,    24] loss: 0.668\n",
      "[1,    32] loss: 0.654\n",
      "[1,    40] loss: 0.650\n",
      "[1,    48] loss: 0.686\n",
      "[1,    56] loss: 0.662\n",
      "[1,    64] loss: 0.700\n",
      "[1,    72] loss: 0.622\n",
      "[1,    80] loss: 0.708\n",
      "[1,    88] loss: 0.686\n",
      "[1,    96] loss: 0.602\n",
      "[1,   104] loss: 0.598\n",
      "[1,   112] loss: 0.657\n",
      "[1,   120] loss: 0.637\n",
      "[1,   128] loss: 0.625\n",
      "[1,   136] loss: 0.636\n",
      "[1,   144] loss: 0.598\n",
      "[1,   152] loss: 0.604\n",
      "[1,   160] loss: 0.607\n",
      "[1,   168] loss: 0.580\n",
      "[1,   176] loss: 0.623\n",
      "[2,     8] loss: 0.633\n",
      "[2,    16] loss: 0.602\n",
      "[2,    24] loss: 0.578\n",
      "[2,    32] loss: 0.576\n",
      "[2,    40] loss: 0.534\n",
      "[2,    48] loss: 0.624\n",
      "[2,    56] loss: 0.651\n",
      "[2,    64] loss: 0.577\n",
      "[2,    72] loss: 0.635\n",
      "[2,    80] loss: 0.616\n",
      "[2,    88] loss: 0.700\n",
      "[2,    96] loss: 0.488\n",
      "[2,   104] loss: 0.484\n",
      "[2,   112] loss: 0.616\n",
      "[2,   120] loss: 0.620\n",
      "[2,   128] loss: 0.553\n",
      "[2,   136] loss: 0.597\n",
      "[2,   144] loss: 0.582\n",
      "[2,   152] loss: 0.587\n",
      "[2,   160] loss: 0.591\n",
      "[2,   168] loss: 0.507\n",
      "[2,   176] loss: 0.597\n",
      "[3,     8] loss: 0.649\n",
      "[3,    16] loss: 0.587\n",
      "[3,    24] loss: 0.548\n",
      "[3,    32] loss: 0.496\n",
      "[3,    40] loss: 0.491\n",
      "[3,    48] loss: 0.686\n",
      "[3,    56] loss: 0.680\n",
      "[3,    64] loss: 0.594\n",
      "[3,    72] loss: 0.632\n",
      "[3,    80] loss: 0.587\n",
      "[3,    88] loss: 0.682\n",
      "[3,    96] loss: 0.476\n",
      "[3,   104] loss: 0.448\n",
      "[3,   112] loss: 0.600\n",
      "[3,   120] loss: 0.657\n",
      "[3,   128] loss: 0.544\n",
      "[3,   136] loss: 0.549\n",
      "[3,   144] loss: 0.568\n",
      "[3,   152] loss: 0.572\n",
      "[3,   160] loss: 0.556\n",
      "[3,   168] loss: 0.483\n",
      "[3,   176] loss: 0.595\n",
      "[4,     8] loss: 0.626\n",
      "[4,    16] loss: 0.561\n",
      "[4,    24] loss: 0.511\n",
      "[4,    32] loss: 0.495\n",
      "[4,    40] loss: 0.462\n",
      "[4,    48] loss: 0.639\n",
      "[4,    56] loss: 0.663\n",
      "[4,    64] loss: 0.545\n",
      "[4,    72] loss: 0.623\n",
      "[4,    80] loss: 0.590\n",
      "[4,    88] loss: 0.742\n",
      "[4,    96] loss: 0.433\n",
      "[4,   104] loss: 0.436\n",
      "[4,   112] loss: 0.555\n",
      "[4,   120] loss: 0.678\n",
      "[4,   128] loss: 0.517\n",
      "[4,   136] loss: 0.496\n",
      "[4,   144] loss: 0.542\n",
      "[4,   152] loss: 0.528\n",
      "[4,   160] loss: 0.534\n",
      "[4,   168] loss: 0.471\n",
      "[4,   176] loss: 0.568\n",
      "[5,     8] loss: 0.650\n",
      "[5,    16] loss: 0.577\n",
      "[5,    24] loss: 0.477\n",
      "[5,    32] loss: 0.523\n",
      "[5,    40] loss: 0.471\n",
      "[5,    48] loss: 0.608\n",
      "[5,    56] loss: 0.649\n",
      "[5,    64] loss: 0.543\n",
      "[5,    72] loss: 0.594\n",
      "[5,    80] loss: 0.630\n",
      "[5,    88] loss: 0.745\n",
      "[5,    96] loss: 0.424\n",
      "[5,   104] loss: 0.383\n",
      "[5,   112] loss: 0.589\n",
      "[5,   120] loss: 0.657\n",
      "[5,   128] loss: 0.501\n",
      "[5,   136] loss: 0.525\n",
      "[5,   144] loss: 0.552\n",
      "[5,   152] loss: 0.491\n",
      "[5,   160] loss: 0.557\n",
      "[5,   168] loss: 0.443\n",
      "[5,   176] loss: 0.515\n",
      "[6,     8] loss: 0.668\n",
      "[6,    16] loss: 0.585\n",
      "[6,    24] loss: 0.505\n",
      "[6,    32] loss: 0.493\n",
      "[6,    40] loss: 0.460\n",
      "[6,    48] loss: 0.612\n",
      "[6,    56] loss: 0.630\n",
      "[6,    64] loss: 0.587\n",
      "[6,    72] loss: 0.568\n",
      "[6,    80] loss: 0.625\n",
      "[6,    88] loss: 0.762\n",
      "[6,    96] loss: 0.408\n",
      "[6,   104] loss: 0.410\n",
      "[6,   112] loss: 0.574\n",
      "[6,   120] loss: 0.628\n",
      "[6,   128] loss: 0.493\n",
      "[6,   136] loss: 0.491\n",
      "[6,   144] loss: 0.502\n",
      "[6,   152] loss: 0.490\n",
      "[6,   160] loss: 0.563\n",
      "[6,   168] loss: 0.425\n",
      "[6,   176] loss: 0.511\n",
      "[7,     8] loss: 0.624\n",
      "[7,    16] loss: 0.609\n",
      "[7,    24] loss: 0.493\n",
      "[7,    32] loss: 0.517\n",
      "[7,    40] loss: 0.476\n",
      "[7,    48] loss: 0.568\n",
      "[7,    56] loss: 0.654\n",
      "[7,    64] loss: 0.585\n",
      "[7,    72] loss: 0.563\n",
      "[7,    80] loss: 0.663\n",
      "[7,    88] loss: 0.740\n",
      "[7,    96] loss: 0.435\n",
      "[7,   104] loss: 0.471\n",
      "[7,   112] loss: 0.676\n",
      "[7,   120] loss: 0.687\n",
      "[7,   128] loss: 0.540\n",
      "[7,   136] loss: 0.504\n",
      "[7,   144] loss: 0.571\n",
      "[7,   152] loss: 0.526\n",
      "[7,   160] loss: 0.595\n",
      "[7,   168] loss: 0.444\n",
      "[7,   176] loss: 0.597\n",
      "[8,     8] loss: 0.612\n",
      "[8,    16] loss: 0.602\n",
      "[8,    24] loss: 0.522\n",
      "[8,    32] loss: 0.511\n",
      "[8,    40] loss: 0.487\n",
      "[8,    48] loss: 0.615\n",
      "[8,    56] loss: 0.642\n",
      "[8,    64] loss: 0.575\n",
      "[8,    72] loss: 0.560\n",
      "[8,    80] loss: 0.609\n",
      "[8,    88] loss: 0.673\n",
      "[8,    96] loss: 0.442\n",
      "[8,   104] loss: 0.437\n",
      "[8,   112] loss: 0.580\n",
      "[8,   120] loss: 0.587\n",
      "[8,   128] loss: 0.505\n",
      "[8,   136] loss: 0.492\n",
      "[8,   144] loss: 0.543\n",
      "[8,   152] loss: 0.467\n",
      "[8,   160] loss: 0.614\n",
      "[8,   168] loss: 0.421\n",
      "[8,   176] loss: 0.514\n",
      "[9,     8] loss: 0.594\n",
      "[9,    16] loss: 0.598\n",
      "[9,    24] loss: 0.521\n",
      "[9,    32] loss: 0.513\n",
      "[9,    40] loss: 0.431\n",
      "[9,    48] loss: 0.583\n",
      "[9,    56] loss: 0.652\n",
      "[9,    64] loss: 0.563\n",
      "[9,    72] loss: 0.504\n",
      "[9,    80] loss: 0.593\n",
      "[9,    88] loss: 0.693\n",
      "[9,    96] loss: 0.396\n",
      "[9,   104] loss: 0.423\n",
      "[9,   112] loss: 0.547\n",
      "[9,   120] loss: 0.611\n",
      "[9,   128] loss: 0.512\n",
      "[9,   136] loss: 0.475\n",
      "[9,   144] loss: 0.481\n",
      "[9,   152] loss: 0.526\n",
      "[9,   160] loss: 0.521\n",
      "[9,   168] loss: 0.422\n",
      "[9,   176] loss: 0.481\n",
      "[10,     8] loss: 0.590\n",
      "[10,    16] loss: 0.593\n",
      "[10,    24] loss: 0.522\n",
      "[10,    32] loss: 0.473\n",
      "[10,    40] loss: 0.425\n",
      "[10,    48] loss: 0.571\n",
      "[10,    56] loss: 0.652\n",
      "[10,    64] loss: 0.551\n",
      "[10,    72] loss: 0.525\n",
      "[10,    80] loss: 0.554\n",
      "[10,    88] loss: 0.664\n",
      "[10,    96] loss: 0.393\n",
      "[10,   104] loss: 0.391\n",
      "[10,   112] loss: 0.562\n",
      "[10,   120] loss: 0.560\n",
      "[10,   128] loss: 0.477\n",
      "[10,   136] loss: 0.445\n",
      "[10,   144] loss: 0.440\n",
      "[10,   152] loss: 0.481\n",
      "[10,   160] loss: 0.499\n",
      "[10,   168] loss: 0.379\n",
      "[10,   176] loss: 0.421\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    running_loss=0.0\n",
    "    for i,data in enumerate(trainloader):\n",
    "        inputs,labels,lengths=data\n",
    "        outputs=model(inputs,lengths)\n",
    "        loss=criteria(outputs,labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss+=loss\n",
    "        if i %8==7:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch+1,i+1,running_loss/8))\n",
    "            running_loss=0.0\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predictions=[]\n",
    "    for data in testloader:\n",
    "        inputs,labels,lengths=data\n",
    "        prediction=model(inputs,lengths)\n",
    "        predictions.extend(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
